{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate video given a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torchaudio\n",
    "import torchaudio.transforms as audio_T\n",
    "from models.model import AVENet\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "class GetSampleFromJson:\n",
    "    def __init__(self, json_file, local_dir):\n",
    "        self.json_file = json_file\n",
    "        self.local_dir = local_dir\n",
    "        with open(json_file, 'r') as f:\n",
    "            data_json = json.load(f)\n",
    "        self.data = data_json['data']\n",
    "        self.image_base_path = data_json['image_base_path']\n",
    "        self.audio_base_path = data_json['audio_base_path']\n",
    "        self._init_transforms()\n",
    "        self.AmplitudeToDB = audio_T.AmplitudeToDB()\n",
    "    \n",
    "    def _init_transforms(self):\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.Resize(224,Image.BICUBIC),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std)\n",
    "        ])\n",
    "\n",
    "    def get_sample(self, index):\n",
    "        sample = self.data[index]\n",
    "        image = os.path.join(self.image_base_path, sample['image'])\n",
    "        audio = os.path.join(self.audio_base_path, sample['wav'])\n",
    "        return image, audio\n",
    "    \n",
    "    def check_sample_in_local(self, index):\n",
    "        img, aud = self.get_sample(index)\n",
    "        img_local = os.path.join(self.local_dir + \"/frame\", img.split('/')[-1])\n",
    "        aud_local = os.path.join(self.local_dir + \"/audio\", aud.split('/')[-1])\n",
    "        return os.path.exists(img_local), os.path.exists(aud_local)\n",
    "    \n",
    "    def download_sample(self, index):\n",
    "\n",
    "        img_rem_path, aud_rem_path = self.get_sample(index)\n",
    "        if all(self.check_sample_in_local(index)):\n",
    "            print(\"Sample already downloaded\")\n",
    "        else:\n",
    "            print(f\"Sample NOT in {self.local_dir} \\n-> download  \")\n",
    "            command = [\"scp\", \"-P\", \"2122\", \"asantos@pirineus3.csuc.cat:\" + img_rem_path, self.local_dir+\"/frame\"]\n",
    "            print(command)\n",
    "            subprocess.run(command)\n",
    "            command = [\"scp\", \"-P\", \"2122\", \"asantos@pirineus3.csuc.cat:\" + aud_rem_path, self.local_dir+\"/audio\"]\n",
    "            print(command)\n",
    "            subprocess.run(command)\n",
    "            \n",
    "        img_local_path = os.path.join(self.local_dir+\"/frame\", img_rem_path.split('/')[-1])\n",
    "        aud_local_path = os.path.join(self.local_dir+\"/audio\", aud_rem_path.split('/')[-1])\n",
    "\n",
    "        return img_local_path, aud_local_path\n",
    "   \n",
    "    def load_image(self, image_path):\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        img = self.img_transform(img)\n",
    "        return img\n",
    "    \n",
    "    def show_image(self, img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        cv2.imshow(\"image\", img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def load_audio_to_spec(self, audio_path):\n",
    "        samples, samplerate = torchaudio.load(audio_path)\n",
    "\n",
    "        if samples.shape[1] < samplerate * 10:\n",
    "            n = int(samplerate * 10 / samples.shape[1]) + 1\n",
    "            samples = samples.repeat(1, n)\n",
    "\n",
    "        samples = samples[...,:samplerate*10]\n",
    "\n",
    "        spectrogram  =  audio_T.MelSpectrogram(\n",
    "                sample_rate=samplerate,\n",
    "                n_fft=512,\n",
    "                hop_length=239, \n",
    "                n_mels=257,\n",
    "                normalized=True\n",
    "            )(samples)\n",
    "        spectrogram =  self.AmplitudeToDB(spectrogram)\n",
    "        return spectrogram\n",
    "    \n",
    "\n",
    "class MatchmapVideoGenerator:\n",
    "    def __init__(self,model, device, img, spec,args, matchmap = None):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.spec = Variable(spec.unsqueeze(0)).to(device, non_blocking=True) if spec.dim() == 3 else Variable(spec).to(device, non_blocking=True)\n",
    "        self.image = Variable(img.unsqueeze(0)).to(device, non_blocking=True) if img.dim() == 3 else Variable(img).to(device, non_blocking=True)\n",
    "        self.args = args\n",
    "        self.matchmap = matchmap\n",
    "        self.video = None\n",
    "\n",
    "    def compute_matchmap(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            img_emb,aud_emb = self.model(self.image.float(), self.spec.float(), self.args)\n",
    "            img_emb = img_emb.squeeze(0)\n",
    "            aud_emb = aud_emb.squeeze(0)\n",
    "            self.matchmap = torch.einsum('ct, chw -> thw',aud_emb,img_emb)\n",
    "        return self.matchmap\n",
    "    \n",
    "    def normalize_img(self, value, vmax=None, vmin=None):\n",
    "        '''\n",
    "        Normalize heatmap\n",
    "        '''\n",
    "        vmin = value.min() if vmin is None else vmin\n",
    "        vmax = value.max() if vmax is None else vmax\n",
    "        if not (vmax - vmin) == 0:\n",
    "            value = (value - vmin) / (vmax - vmin)  # vmin..vmax\n",
    "        return value\n",
    "    \n",
    "    def get_frame_match(self, img_np, matchmap_np, frame_idx):\n",
    "        assert img_np.ndim == 3, \"img_np should be a 3D numpy array\"\n",
    "        assert matchmap_np.ndim == 3, \"matchmap_np should be a 3D numpy array\"\n",
    "\n",
    "        matchmap_i = matchmap_np[frame_idx]\n",
    "        matchmap_i = cv2.resize(matchmap_i, dsize=(224, 224), interpolation=cv2.INTER_LINEAR)\n",
    "        matchmap_i = self.normalize_img(matchmap_i)\n",
    "        matchmap_i_photo = (matchmap_i * 255).astype(np.uint8)\n",
    "        matchmap_i_photo = cv2.applyColorMap(matchmap_i_photo, cv2.COLORMAP_JET)\n",
    "        matchmap_i_photo = cv2.addWeighted(matchmap_i_photo, 0.5, img_np, 0.5, 0)\n",
    "        return matchmap_i_photo\n",
    "    \n",
    "    def create_video_f(self,img_np, matchmap_np, output_path=\"matchmap_video.mp4\", fps=1):\n",
    "        n_frames = matchmap_np.shape[0]\n",
    "        \n",
    "        # Make sure img_np is in uint8 format\n",
    "        if img_np.dtype != np.uint8:\n",
    "            img_np = (img_np * 255).astype(np.uint8)\n",
    "        \n",
    "        # Make sure img_np has correct dimensions (224, 224, 3)\n",
    "        if img_np.shape[:2] != (224, 224):\n",
    "            img_np = cv2.resize(img_np, (224, 224))\n",
    "        \n",
    "        # Use proper codec for compatibility\n",
    "        # For better compatibility, try 'avc1' or 'H264' instead of 'mp4v'\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'avc1')  # Try this first\n",
    "        \n",
    "        # Alternative codec options if 'avc1' doesn't work:\n",
    "        # fourcc = cv2.VideoWriter_fourcc(*'H264')\n",
    "        # fourcc = cv2.VideoWriter_fourcc(*'XVID')  # More compatible but lower quality\n",
    "        \n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (224, 224))\n",
    "        \n",
    "        if not out.isOpened():\n",
    "            print(\"Failed to create VideoWriter. Trying alternative codec...\")\n",
    "            # Try with different codec\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "            out = cv2.VideoWriter(output_path.replace('.mp4', '.avi'), fourcc, fps, (224, 224))\n",
    "        \n",
    "        for i in range(n_frames):\n",
    "            frame = self.get_frame_match(img_np, matchmap_np, i)\n",
    "            \n",
    "            # Ensure frame is the correct format\n",
    "            if frame.dtype != np.uint8:\n",
    "                frame = (frame * 255).astype(np.uint8)\n",
    "                \n",
    "            # Ensure frame has the right shape\n",
    "            if frame.shape[:2] != (224, 224):\n",
    "                frame = cv2.resize(frame, (224, 224))\n",
    "                \n",
    "            # Verify frame is BGR (OpenCV's default format)\n",
    "            if len(frame.shape) == 3 and frame.shape[2] == 3:\n",
    "                out.write(frame)\n",
    "            else:\n",
    "                print(f\"Warning: Frame {i} has incorrect format. Shape: {frame.shape}\")\n",
    "        \n",
    "        out.release()\n",
    "        print(f\"Video created at: {output_path}\")\n",
    "        \n",
    "        # Verify the file was created and has a non-zero size\n",
    "        import os\n",
    "        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
    "            print(f\"Success! Video file created: {os.path.getsize(output_path)} bytes\")\n",
    "        else:\n",
    "            print(\"Error: Video file was not created properly\")\n",
    "            \n",
    "    def create_video(self,output_path):\n",
    "        img_np = self.image[0].cpu().numpy()\n",
    "        img_np = np.transpose(img_np, (1, 2, 0))\n",
    "        img_np = self.normalize_img(img_np)\n",
    "        img_np = (img_np * 255).astype(np.uint8)\n",
    "        img_np = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if self.matchmap is None:\n",
    "            self.matchmap = self.compute_matchmap()\n",
    "\n",
    "        matchmap_np = self.matchmap.cpu().numpy()\n",
    "        n_frames = matchmap_np.shape[0]\n",
    "        self.create_video_f(img_np, matchmap_np, output_path, fps=n_frames/10) # 10 sec duration\n",
    "    \n",
    "\n",
    "def load_model(ckpt_path, args):\n",
    "    model = AVENet(args)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.to(device)\n",
    "    return model, device\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14439/2851987544.py:28: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  transforms.Resize(224,Image.BICUBIC),\n",
      "/home/albert/miniconda3/envs/ssl-cpu/lib/python3.8/site-packages/torchaudio/functional/functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (257) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample already downloaded\n",
      "Video created at: dir_exp_PlacesAudio/mm_lr1e-5_2layersB256_epoch100_train_234.mp4\n",
      "Success! Video file created: 76884 bytes\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from opts import get_arguments\n",
    "import torch\n",
    "\n",
    "# Simulate command-line arguments\n",
    "sys.argv = ['script_name', '--order_3_tensor', '--simtype', 'MISA']\n",
    "\n",
    "args = get_arguments()\n",
    "\n",
    "#JSON\n",
    "split = \"train\"\n",
    "json_file = f\"garbage/{split}.json\"\n",
    "local_dir_saving = \"dir_exp_PlacesAudio\"\n",
    "\n",
    "# Load model\n",
    "# model_name = \"2layers_lr1e-5_epoch100\"\n",
    "# model_name = \"2layers_lr1e-5_epoch1\"\n",
    "model_name = \"lr1e-5_2layersB256_epoch100\"\n",
    "\n",
    "checkpoint_path = f'garbage/{model_name}.pth.tar'\n",
    "model,device = load_model(checkpoint_path, args)\n",
    "\n",
    "#SAMPLE\n",
    "sample_idx = 234\n",
    "\n",
    "# Get the image and audio\n",
    "gs = GetSampleFromJson(json_file,local_dir_saving)\n",
    "img_local_path, aud_local_path = gs.download_sample(sample_idx)\n",
    "\n",
    "img = gs.load_image(img_local_path)\n",
    "spec = gs.load_audio_to_spec(aud_local_path)\n",
    "\n",
    "# Generate the video\n",
    "mgv = MatchmapVideoGenerator(model,device,img,spec,args)\n",
    "mgv.create_video(local_dir_saving+ f\"/mm_{model_name}_{split}_{sample_idx}.mp4\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssl-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
